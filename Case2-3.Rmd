---
title: "Untitled"
author: "Nathaniel Brown, In Hee Ho, Sarah Zimmermann"
date: "October 19, 2017"
output:
  pdf_document: default
  html_document: default
---

```{r, warning=FALSE, echo=FALSE, message=FALSE}
set.seed(440)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message=FALSE)
library(knitr)
library(ggplot2)
library(survival)
library(gridExtra)
library(dplyr)
library(survminer)
library(reshape2)
library(glmnet)
library(arm)
library(mice)
```

```{r, warning=FALSE, echo=FALSE, message=FALSE}
dat <- read.table("kellydat.txt", header=T)
dat$race = "Other"
dat$race[dat$black==1|dat$hisp==1 ]="Black or Hispanic"
dat$gender = "male"
dat$gender[dat$male==0]="female"
dat$symptom = "0"
dat$symptom[dat$sn1==1]="1"
dat$symptom[dat$sn2==1]="2"
dat$symptom[dat$sn3==1|dat$all4==1]="3+"
dat$scan="not scanned"
dat$scan[dat$fail==1]="scanned"
dat = dat %>% group_by(race) %>% mutate(racescan = ifelse(fail == 1, mean(fail), 1-mean(fail)))
dat = dat %>% group_by(symptom) %>% mutate(symptomscan = ifelse(fail == 1, mean(fail), 1-mean(fail)))
dat = dat %>% group_by(gender) %>% mutate(genderscan = ifelse(fail == 1, mean(fail), 1-mean(fail))) %>% as.data.frame()
```

## Introduction

The data are from a study of time to critical neurological assessment for patients with stroke-like symptoms who are admitted to the emergency room. We are interested in the factors predictive of the time to assessment following admission to the ED for n=335 patients with mild to moderate motor impairment. The goal of the analysis is to perform inferences on the impact of clinical presentation, gender, and race (Black, Hispanic, and others) on time to neurological assessment, where clinical presentation is measured as the number of the four major stroke symptoms: headache, loss of motor skills or weakness, trouble talking or understanding, and vision problems. However, as discussed in our previous report, we group Blacks and Hispanics together, and number of symptoms of 3 and 4 together, due to their small sample size.

##Methods

The team has cleaned, understood, and modeled these time to critical neurological assessment for patients with stroke-like symptoms data in order to solve the scientific problem of exploring if gender, race/ethnicity, and clinical presentation have an affect on wait list to assessment. To do so the team has approached the problem as such:

1. Data Exploration 
  + Read in the data 
  + Explore summary statistics of data 
  + Visualize the data 
2. Create inital models including OLS, Ridge, and LASSO
  + Diagnostics
  + Results
  + Survival Curves 
  + Interpretation 
  + Assess sucess
3. Create final models with kernel regression
  + Diagnostics
  + Results
  + Survival Curves 
  + Interpretation
4. Final recommendations and insight 

##Data Exploration

###Variables 

The original data set contains 335 observations across 9 variables. They are defined as: 


Variable Name | Short Description | Type  
------------- | ------------- | --------------
nctdel|min of neurologist time to assessment & CT scan from arrival at ER| continous
fail|1 if got neurologist/CT scan & 0 otherwise|categorical
male| 1 if male, 0 if female| categorical
black|1 if black, 0 if not black|categorical
hisp|1 if hispanic, 0 if not hispanic|categorical
sn1|0/1 indicator 1 main symptom| categorical
sn2|0/1 indicator 2 main symptoms| categorical
sn3|0/1 indicator 3 main symptoms| categorical
all4|0/1 indicator all main sumptoms|categorical


```{r}
pdat = dat[,1:2]
pdat$race = "Others"
pdat$race[dat$black==1]="Black"
pdat$race[dat$hisp==1]="Hispanic"
pdat$gender = "male"
pdat$gender[dat$male==0]="female"
pdat$symptom = "0"
pdat$symptom[dat$sn1==1]="1"
pdat$symptom[dat$sn2==1]="2"
pdat$symptom[dat$sn3==1]="3"
pdat$symptom[dat$all4==1]="4"

p1<- ggplot(data = pdat, aes(x=race, fill=race)) + geom_bar() + theme_bw()+labs(x="Race", y="Frequency",fill="") + theme(axis.text = element_text(angle = 45, hjust = 1))
p2<- ggplot(data = pdat, aes(x=symptom, fill=symptom)) + geom_bar() + theme_bw()+labs(x="Number of Symptoms", y="Frequency", fill="")
p3<- ggplot(data= pdat, aes(x=gender, fill= gender)) + geom_bar() + theme_bw()+labs(x="Gender", y="Frequency", fill="")

grid.arrange(p1, p2, p3 , ncol=3)

```

To visualize the data we have provided the above graphs which show the frequency of the characteristics patients can have. We see most (above 250) of the patients are non-black and non hispanic  and less than 100 are either black or hispanic. The most common number of symptoms is 1 then 0, 2, 3, and 4. Finally the gender split between male and females is generally even; however, there are more females in the data then male. 

Before moving on it should be noted there are no missing values or apparently out of range values in our data, and therefore we did not clean the data in any way. We did group some of the characteristics of patients because of sample size, however. In the following analysis we grouped black and hispanics in one category for race/ethnicity and non black and non hispanics in the other. This decision was based on the fact there are only 9 hispanics in the data set which is too small of a population to make meaningful conclusions.  Next, symptom is a 4 level variable: "0" for those who had no symptoms, "1" for those who had 1 symptom, "2" for those who had 2 symptoms, and "3+" for those who had 3 or more symptoms. We grouped those patients which 3 or 4 symptoms together because there were only 6 individuals out of 335 observations in the dataset who had 4 symptoms, which is very small a population size to draw any conclusions on.  

We now continue on in our report to further understand the data specifically the exploratory data analysis we found useful when later we created data models. 


###Exploratory Data Analysis


#PUT USEFUL EDA HERE 



```{r logitfunctions}
#all the functions

lower_bound <- function(x, bounds){
  ret <- rep(NA, length(x))
  for(i in 1:length(x)){
    xi <- x[i]
    found <- (which(xi >= c(-Inf, bounds) & (xi <= c(bounds, Inf))))[1] - 1
    if(found==0){found<-1}
    ret[i] <- bounds[found]
  }
  return(ret)
}

de_logit <- function(logodds){
  o <- exp(logodds)
  p <- o/(1+o)
  return(p)
}

#transform continuous time (nctdel) into categries based on lower bounds of "bounds" variable
categorize_dat <- function(dat=NA, bounds=NA){

  dat$timecat <- as.integer(as.factor(lower_bound(dat$nctdel, bounds)))
  dat$personid <- 1:nrow(dat)
  datcat <- merge(dat$personid, bounds) %>% 
          mutate(personid=x, timecat=as.integer(as.factor(y))) %>%
            #take all unique combinations of people and time categories
          merge(dat, by=c("personid", "timecat"), all=TRUE) %>%
          mutate(fail = ifelse(is.na(fail), 0, fail)) %>%
            #add fail column
          group_by(personid) %>%
          mutate(maxtimecat = (timecat)[!is.na(nctdel)],
                 atrisk = (timecat <= maxtimecat)) %>%
          filter(atrisk) %>%
            #for each person, identify rows where persons are not at risk (after they fail/drop out), and remove those rows
          dplyr::select(personid, timecat, fail) %>%
          as.data.frame()

#code to add predictors of race, gender, and number of symptoms
  datcat <- merge(datcat, dat, by="personid", all=T) %>% 
          mutate(symptom0 = ifelse(symptom == 0, 1, 0),
                 symptom1 = ifelse(symptom == 1, 1, 0),
                 symptom2 = ifelse(symptom == 2, 1, 0),
                 raceother = ifelse(race == "Other", 1, 0), 
                 male = ifelse(gender == "male", 1, 0), 
                 personid = as.integer(personid), 
                 timecat = as.integer(timecat.x), 
                 fail = as.integer(fail.x)) %>% 
          dplyr::select(personid, timecat = timecat, fail = fail, symptom0, symptom1, symptom2, raceother, male)

#code to add indicator effects of every timeunit
  Xmat <- array(0, c(nrow(datcat), length(bounds)))

  for(r in 1:nrow(datcat)){
    Xmat[r,datcat$timecat[r]] <- 1
  }

  colnames(Xmat) <- paste0("X",1:ncol(Xmat))


  datcat_X <- (cbind((datcat), Xmat))

  datcat_X$nctdel <- merge(datcat_X, dat, by="personid", all=TRUE)$nctdel
  
  return(datcat_X)
}

#obtain fitted values, pearson residuals, deviance residuals, and other diagnostic tools
logistic_diagnostics <- function(mod=NA, ydat=y, Xdat=X){
  
  if(class(mod)[1] == "cv.glmnet"){
    fitvals <- predict.cv.glmnet(mod, newx = Xdat, s="lambda.min")
  }else{
    fitvals <- predict.glm(mod, newdata=data.frame(Xdat))
  }
  p_hat <- de_logit(fitvals)
  resids_p <- (ydat-p_hat)/(sqrt(p_hat*(1-p_hat)))
  s <- ydat; s[ydat==0] <- -1
  resids_d <- s*sqrt(-2*(ydat*log(p_hat) + (1-ydat)*log(1-p_hat)))
  resids_p_stat <- sum(resids_p^2)
  resids_d_stat <- -2*sum(ydat*log(p_hat) + (1-ydat)*log(1-p_hat))
  p <- sum(as.matrix(coef(mod)) != 0 | is.na(as.matrix(coef(mod))))
  dof <- nrow(Xdat)-p
  return(list(p_hat = p_hat, resids_p=resids_p, resids_p_stat=resids_p_stat, resids_d=resids_d, resids_d_stat=resids_d_stat, dof=dof))
}

#obtain confidence intervals for coefficients (for glm object)
logistic_conf <- function(mod){
  stats <- summary(mod)$coefficients
  xbar <- stats[,1]
  sigma <- stats[,2]
  logmodcoef <- xbar + 1.96*cbind(-sigma, sigma)
  logmodcoef <- round(logmodcoef,4)
  colnames(logmodcoef) <- c("Lower", "Upper")
  return(logmodcoef)
}

#reshape categorized timebins into a specified number of kernels
kernel_transformation <- function(datcat, bounds, kernels, s=NA) {

  Ks <- array(NA, c(nrow(datcat), kernels))
  colnames(Ks) <- paste0("k", 1:kernels)
  datcat <- datcat[,!startsWith(colnames(datcat), "X")]
  datcat_k <- cbind(datcat, Ks)
  kcols <- (ncol(datcat_k) - kernels + 1):ncol(datcat_k)
  
  tau <- seq(from=1, to=max(bounds), length.out = kernels)
  if(is.na(s)){
    s <- (tau[2]-tau[1])/2
  }
  # each row is one bin, the columns are the kernel weights for that bin
  kernel.weights <- matrix(dnorm(x=rep(1:length(bounds),kernels),mean=rep(tau,each=length(bounds)),s), ncol=kernels)

  for(p in unique(datcat$personid)){
    subrows <- which(datcat$personid == p)
    time <- max(datcat$timecat[subrows])

    K <- array(kernel.weights[1:time,],dim=c(time,kernels))
    datcat_k[subrows,kcols] <- K
  }
  #datcat_k <- cbind(datcat, K)
  # X has kernel weights instead of the bin indicators that it had before
   return(as.data.frame(datcat_k))
}


```

```{r logitanalysis}
#all the analysis

bounds <- 0:5
datcat_X <- categorize_dat(dat, bounds)
indepcols <- which(!colnames(datcat_X) %in% c("fail", "personid", "timecat", "nctdel"))
X <- as.matrix(datcat_X[,indepcols])
y <- datcat_X$fail

kernels <- 2
datcat_Xk <- kernel_transformation(datcat_X, bounds, kernels, s=NA)
indepcolsk <- which(!colnames(datcat_Xk) %in% c("fail", "personid", "timecat", "nctdel"))
Xk <- as.matrix(datcat_Xk[,indepcolsk])
yk <- datcat_Xk$fail

nullmod <- glm(fail ~ 1, data=datcat_X, family="binomial")
nullmod_stats <- logistic_diagnostics(nullmod)
nullmod_test <- pchisq(nullmod$deviance, nullmod$df.residual, lower=FALSE)

logmod <- glm(y ~ 0 + ., family="binomial", data = as.data.frame(cbind(y,X))) #0 means no incercept
logmod_stats <- logistic_diagnostics(logmod)
logmod_test <- pchisq(logmod$deviance, logmod$df.residual, lower=FALSE)
logmod_coef <- logistic_conf(logmod)

lassomod <- cv.glmnet(y=y, x = X, family="binomial", type.measure="class", alpha = 1, intercept=FALSE)
lassomod_stats <- logistic_diagnostics(lassomod)
lassomod_test <- pchisq(lassomod_stats$resids_d_stat, lassomod_stats$dof, lower=FALSE)
lassomod_coef <- round(as.matrix(coef(lassomod)),4)
colnames(lassomod_coef) <- "LASSO Estimate"

ridgemod <- cv.glmnet(y=y, x = X, family="binomial", type.measure="class", alpha = 0,intercept=FALSE)
ridgemod_stats <- logistic_diagnostics(ridgemod)
ridgemod_test <- pchisq(ridgemod_stats$resids_d_stat, ridgemod_stats$dof, lower=FALSE)
ridgemod_coef <- round(as.matrix(coef(ridgemod)),4)
colnames(ridgemod_coef) <- "Ridge Estimate"

kernelmod <- glm(yk ~ 0 + .,  data = as.data.frame(cbind(yk,Xk)), family="binomial")
kernelmod_stats <- logistic_diagnostics(kernelmod, yk, Xk)
kernelmod_test <- pchisq(kernelmod_stats$resids_d_stat, kernelmod_stats$dof, lower=FALSE)
kernelmod_coef <- logistic_conf(kernelmod)

```



##Initial Model Exploration

Now that we have visualized each variable along with the relationship between the variables we will continue on to modeling our data. 

To perform logistic regression on the data, we categorize the time-to-event variable into groups of 1 minute,
with all events occuring after 5 minutes grouped together, since the sample size is low after 5 minutes, with
only 9 observations. Our predictors consist of these time categories, as well as the aforementioned categories
of race, gender, and clinical presentation. The binned residual plots, deviance test results, and coefficients are
reported below. We attempt three approaches to logistic regresion: Ordinary Least Squares (OLS), LASSO,
and Ridge. The glmnet package does not provide standard errors for its coefficients, so we cannot report
confidence intervals for the estimates.

###Diagnostics

We check the diagnostics to see if the model properly fits the data.The residual plots suggest a poor fit for all the models other than the OLS model because of the lack of even spread of the residuals. The logistic models require normality and independence and from the residuals we cannot fully say 

```{r}

par(mfrow=c(2,2))

binnedplot(x=logmod$fitted.values, y=logmod_stats$resids_d, main = "OLS Log Regression Binned Resid", ylab = "Deviance Residuals", xlab = "Fitted Values", nclass = 50)

binnedplot(x=lassomod_stats$p_hat, y=lassomod_stats$resids_d, main = "LASSO Log Regression Binned Resid", ylab = "Deviance Residuals", xlab = "Fitted Values", nclass = 50)

binnedplot(x=ridgemod_stats$p_hat, y=ridgemod_stats$resids_d, main = "Ridge Log Regression Binned Resid", ylab = "Deviance Residuals", xlab = "Fitted Values", nclass = 50)


```

###Results 

Below are the results of the OLS, LASSO Penalty and Ridge Penalty respectively.

All three of our logistic regression approaches-OLS, LASSO, and Ridge-do not find much difference between
these groups. In the OLS model, the only coefficients that do not contain zero in their 95% confidence interval
are X1 and X3. In the LASSO and Ridge models, the majority of coefficients are close to zero or exactly zero,
and all of the models fit the data poorly according to the deviance test. Therefore, we cannot confidently claim that any of
these factors are predictive of the time to assessment.

```{r}
d <- round(matrix(c(logmod_test, lassomod_test, ridgemod_test)),3)
colnames(d) <- "Deviance p-value"
rownames(d) <- c("OLS", "LASSO Penalty", "Ridge Penalty")


kable(d)

kable(list(logmod_coef, lassomod_coef, ridgemod_coef))
```

###Survival Curves 
```{r}
#survival to time t = p(x>t);
get_timecat_hazards <- function(mod){
  coefs <- as.matrix(coef(mod))
  invlogit(coefs[which(startsWith(rownames(coefs),"X")),])
}

get_timecat_survival <- function(mod_hazards){
  p <- s <- NULL
  for(t in 1:length(mod_hazards)){

    #h_1 <- p(x=1)/p(x>=1) = p(x=1)
    #h_2 <- p(x=2)/p(x>=2) = p(x=2)/(1-h_1)
    #h_3 <- p(x=3)/p(x>=3) = p(x=3)/[(1-h_1)(1-h_2)]
  
    #h_t <- p(x=t)/[(1-h_1)(1-h_2)...(1-h_(t-1))] ==> p(x=t) = h_t(1-h_1)(1-h_2)...(1-h_(t-1))
      p[t] <- prod(1-mod_hazards[0:(t-1)])*mod_hazards[t]
  #calculate discrete pdf
  
    #put the p(x=1), p(x=2)...all together

  #calculate survival by summing pdf bins
  
    #s(t) = sum(pdf) from 0 to t
  }
  s <- 1-cumsum(p)
  return(list(pmf=p, surv=s))
}


#calculate the hazards of each time category (kernels are a little more complicated)
logmod_hazards <- get_timecat_hazards(logmod)
lassomod_hazards <- get_timecat_hazards(lassomod)
ridgemod_hazards <- get_timecat_hazards(ridgemod)



logmod_surv <- get_timecat_survival(logmod_hazards)
lassomod_surv <- get_timecat_survival(lassomod_hazards)
ridgemod_surv <- get_timecat_survival(ridgemod_hazards)


par(mfrow=c(1,3))
plot(x=1:length(logmod_surv[[2]]), y=logmod_surv[[2]], type="s")
plot(x=1:length(lassomod_surv[[2]]), y=lassomod_surv[[2]], type="s")
plot(x=1:length(ridgemod_surv[[2]]), y=ridgemod_surv[[2]], type="s")
```

#CREATE INDIVIDUAL SURVIVAL CURVES HERE

###Interpretation 

The models we build for this analysis do not fit the data well. This can be due to a relatively small sample
size of 335 patients, or the possibility that there is no measurable difference between races, genders, and
clinical presentation in time to treatment. In future analysis, we will attempt to fit more flexible models,
such as generalized additive models with kernel smoothing.



##Final Model Exploration 

describe kernel regression 


```{r}
binnedplot(x=kernelmod_stats$p_hat, y=kernelmod_stats$resids_d, main = "Kernel Logistic Regression Binned Residuals", ylab = "Deviance Residuals", xlab = "Fitted Values", nclass = 50)
```



##Results  

The coefficients of this model suggest wait time changes with number of symptoms, race, and gender; however, since the coefficients are small and close to 0 they do not suggest that much influence on wait time. Nevertheless, the more the symtoms the shorter the wait time, if the patient is non black or non hispanic the wait time will be shorter, and finally if the patient is male the wait time is expected to be shorter as well. 

We will now visualize these results by creating a survival curves for the total population and survival curves for patients with 0/1/2/3+ symptoms, male/female, and black+hispanic/non-black+ non-hispanic.  

```{r}
kable(kernelmod_coef)
```


###Survival Curves 

Below is the survival curve for the entire population despite race, gender, and clinical presentation. We will compare the population survival curve to the survival curves based on gender, race, and clinical presentation. If there is a difference between the total population survival curve and the other survival curves this is a way to tell if gender, race, or clinical presentation have an influence on wait time. 

```{r}

#kernel survival curve 
bounds=c(0,1,2,3,4,5)
kernels=2
tau <- seq(from=1, to=max(bounds), length.out = kernels)
s <- (tau[2]-tau[1])/2
kernel.weights <- matrix(dnorm(x=rep(1:length(bounds),kernels),mean=rep(tau,each=length(bounds)),s), ncol=kernels)

kernel.weights_1= data.frame(kernel.weights, rep(-0.73895618, 6), rep(-0.35687333, 6), rep(-0.44156633), rep(0.05489664 ,6), rep(-0.25308262, 6) )

colnames= c("k1", "k2", "symptom0", "symptom1", "symptom2", "raceother", "male")

colnames(kernel.weights_1)= colnames

x= predict(kernelmod, kernel.weights_1)


kernel_surv= get_timecat_survival(invlogit(x))
plot(x=1:length(kernel_surv[[2]]), y=kernel_surv[[2]], type="s")

ksurv_k1= get_timecat_survival(c(invlogit(x)[1], 1,1,1,1,1))
ksurv_k2= get_timecat_survival(c(1, invlogit(x)[2],1,1,1,1))
ksurv_symp0= get_timecat_survival(c(1, 1,invlogit(x)[3],1,1,1))
ksurv_symp2= get_timecat_survival(c(1, 1,1,invlogit(x)[4],1,1))
ksurv_raceother= get_timecat_survival(c(1, 1,1,1,invlogit(x)[5],1))
ksurv_male= get_timecat_survival(c(invlogit(x)[1], 1,1,1,1,invlogit(x)[6]))

plot(x=1:length(ksurv_k1[[2]]), y=ksurv_k1[[2]], type="s", col="red")
plot(x=1:length(ksurv_k2[[2]]), y=ksurv_k2[[2]] ,  type="s", col="blue")
plot(x=1:length(ksurv_symp0[[2]]), y=ksurv_symp0[[2]],  type="s", col="green")
plot(x=1:length(ksurv_symp2[[2]]), y=ksurv_symp2[[2]], type="s", col="black")
plot(x=1:length(ksurv_raceother[[2]]), y=ksurv_raceother[[2]],  type="s",col="magenta")
plot(x=1:length(ksurv_male[[2]]), y=ksurv_male[[2]],  type="s",col="orange")



```
###Interpretation 





# Discussion

why nothing is significant:

```{r}
#this is not actually part of the report as of right now
dat$lnctdel <- log(dat$nctdel+0.01)
dat %>% filter(nctdel < quantile(nctdel,0.95) & fail == 1) %>% group_by(symptom) %>% summarize(mean = mean(nctdel), n = n(), sd = sd(nctdel)) %>% mutate(lower = (mean - 1.96*sd/sqrt(n)), upper = (mean + 1.96*sd/sqrt(n))) 

dat %>% filter(nctdel < quantile(nctdel,0.95) & fail == 1) %>% group_by(gender) %>% summarize(mean = mean(nctdel), median = median(nctdel))

dat %>% filter(nctdel < quantile(nctdel,0.95) & fail == 1) %>% group_by(race) %>% summarize(mean = mean(nctdel), median = median(nctdel))

```

##References 

https://www.r-bloggers.com/imputing-missing-data-with-r-mice-package/

http://influentialpoints.com/Training/coxs_proportional_hazards_regression_model-principles-properties-assumptions.htm#modmch


https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3059453/


http://dwoll.de/rexrepos/posts/survivalKM.html

##Credits 





















Old survival curves.. not sure if we need these 
# ```{r}
# plot(survfit(Surv(timecat, fail) ~ raceother + male, data = datcat_X), 
#      main=expression(paste("Kaplan-Meier Estimate ", hat(S)(t), " with CI")),xlab="t", ylab="Survival", lwd=2)
# ```
# 
# ```{r}
# plot(survfit(Surv(timecat, fail) ~ raceother + male, data=datcat_X) , xlab="Survival Time", 
#   ylab="% Surviving", yscale=100, col=c("red","blue", "black", "green"),
#   main="Survival Distributions") 
#   legend("topright", title="Legend", c("Black/Hisp", "Non Black/Hisp" ,"Male", "Female"),
#   fill=c("red", "blue", "black", "green"))
#   
#   survdiff(Surv(timecat, fail) ~ raceother + male, data=datcat_X)
# ```



Mice stuff 

# ```{r}
# data <- read.table("kellydat.txt", header=T)
# data$race = 0
# data$race[data$black==1|data$hisp==1] = 1
# data$sn0 = 0
# data$sn0[data$sn1==0 & data$sn2==0 & data$sn3==0 & data$all4==0] = 1
# 
# data.imp = data
# data.imp$nctdel[data.imp$fail == 0] = NA
# 
# #md.pattern(data.imp[!is.na(data.imp$nctdel),])
# 
# tempData <- mice(data.imp,m=5,maxit=50,meth='pmm',seed=500, print=FALSE)
# # methods: 
# # 2l.norm / 2l.pan / 2lonly.mean / 2lonly.norm / 2lonly.pmm / 
# # cart / fastpmm / lda / logreg / logreg.boot / mean / midastouch / norm / norm.boot / norm.nob / 
# # norm.predict / passive / pmm / polr / polyreg / quadratic / rf / ri / sample
# 
# #tempData$imp$nctdel
# 
# data.imp <- complete(tempData)
# 
# hist(log(data.imp$nctdel + 0.1))
# 
# fit <-lm(log(nctdel+0.1) ~ sn0 + sn1 + sn2 + race + male, data = data)
# ```
# 
# ##Diagnostic 
# 
# 
# 
# 
# 
# 
# 
# ```{r}
# library(VIM)
# library(mice)
# 
# 
# ##looking for pattern of missing data
# md.pattern(data.imp)
# 
# #visualizations: 
# aggr_plot <- aggr(data, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(data), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))
# 
# marginplot(data.imp[c(1,2)])
# marginplot(data[c(1,3)])
# marginplot(data[c(1,4)])
# marginplot(data[c(1,5)])
# marginplot(data[c(1,6)])
# marginplot(data[c(1,7)])
# marginplot(data[c(1,8)])
# marginplot(data[c(1,9)])
# marginplot(data[c(1,10)])
# 
# 
# 
# #visualize distribution of original and imputed data- check if imputed data is plausible
# 
# #xyplot(tempData,nctdel ~ fail+male+black,pch=18,cex=1)  
# 
# #densityplot(tempData)
# 
# #stripplot(data, pch = 20, cex = 1.2)
# ```

